import logging
import sqlite3
from collections import Counter
from typing import Dict, List, Optional, Set, Tuple
from urllib.parse import urlparse
import os


class DomainClassifier:
    """Classifies domains based on predefined service mappings."""

    def __init__(self) -> None:
        self._service_domains: Dict[str, str] = {
            'userapi.com': 'VK / userapi',
            'vkuserphoto.ru': 'VK / userapi',
            'mycdn.me': 'VK / userapi',
            'okcdn.ru': 'OK.ru',
            'googleusercontent.com': 'Google',
            'ytimg.com': 'Google',
            'yt3.googleusercontent.com': 'Google',
            'blogger.com': 'Google',
            'staticflickr.com': 'Flickr',
            'my.mail.ru': 'Mail.ru / My.Mail',
            'foto.my.mail.ru': 'Mail.ru / My.Mail',
            'fotokto.ru': 'Fotokto.ru',
            '.mt.ru': 'MT.ru',
            'yandex.net': 'Yandex',
            'yandex.ru': 'Yandex',
            'img-fotki.yandex.ru': 'Yandex',
            'avatars.dzeninfra.ru': 'Yandex',
            'icdn.ru': 'ICDN.ru',
            'imgbb.ru': 'ImgBB',
            'pinimg.com': 'Pinterest',
            'behance.net': 'Behance',
            'wikimedia.org': 'Wikimedia',
            'livejournal.com': 'LiveJournal',
            'imgur.com': 'Imgur',
            'mm.bing.net': 'Bing',
        }

        # Domains that require substring matching
        self._service_substrings: Dict[str, str] = {
            'mail.ru': 'Mail.ru / My.Mail',  # Special case: check for avt- or filed in domain
        }

    def classify(self, domain: str) -> str:
        """
        Classifies a domain into a service or returns the domain itself if not classified.

        Args:
            domain: The domain to classify.

        Returns:
            The service name if classified, otherwise the original domain.
        """
        domain_lower = domain.lower()

        # Check for exact matches first
        for service_domain, service_name in self._service_domains.items():
            if service_domain.startswith('.'):
                # Domain suffix match (e.g., .mt.ru)
                if domain_lower.endswith(service_domain):
                    return service_name
            elif service_domain == domain_lower:
                return service_name

        # Check for substring matches
        for service_substring, service_name in self._service_substrings.items():
            if service_substring in domain_lower:
                if service_substring == 'mail.ru':
                    if 'avt-' in domain_lower or 'filed' in domain_lower:
                        return service_name

        # Special substring checks
        if any(x in domain_lower for x in ['userapi.com', 'vkuserphoto.ru', 'mycdn.me']):
            return 'VK / userapi'
        if 'okcdn.ru' in domain_lower:
            return 'OK.ru'
        if any(x in domain_lower for x in [
            'googleusercontent.com',
            'ytimg.com',
            'yt3.googleusercontent.com',
            'blogger.com'
        ]):
            return 'Google'
        if 'staticflickr.com' in domain_lower:
            return 'Flickr'
        if any(x in domain_lower for x in ['my.mail.ru', 'foto.my.mail.ru']) or (
                'mail.ru' in domain_lower and ('avt-' in domain_lower or 'filed' in domain_lower)):
            return 'Mail.ru / My.Mail'
        if 'fotokto.ru' in domain_lower:
            return 'Fotokto.ru'
        if domain_lower.endswith('.mt.ru'):
            return 'MT.ru'
        if any(x in domain_lower for x in [
            'yandex.net',
            'yandex.ru',
            'img-fotki.yandex.ru',
            'avatars.dzeninfra.ru'
        ]):
            return 'Yandex'
        if 'icdn.ru' in domain_lower:
            return 'ICDN.ru'
        if 'imgbb.ru' in domain_lower:
            return 'ImgBB'
        if 'pinimg.com' in domain_lower:
            return 'Pinterest'
        if 'behance.net' in domain_lower:
            return 'Behance'
        if 'wikimedia.org' in domain_lower:
            return 'Wikimedia'
        if 'livejournal.com' in domain_lower:
            return 'LiveJournal'
        if 'imgur.com' in domain_lower:
            return 'Imgur'
        if 'mm.bing.net' in domain_lower:
            return 'Bing'

        return domain  # Return original domain if no match


class ImageUrlAnalyzer:
    """Analyzes image URLs from a database and provides classification statistics."""

    def __init__(self, db_path: str) -> None:
        self.db_path = db_path
        self.classifier = DomainClassifier()
        self.logger = logging.getLogger(__name__)

    def _connect_to_database(self, path: str = None) -> Tuple[sqlite3.Connection, sqlite3.Cursor]:
        """Establishes a connection to the SQLite database."""
        db_to_connect = path if path else self.db_path
        try:
            conn = sqlite3.connect(db_to_connect)
            cursor = conn.cursor()
            self.logger.info(f"Successfully connected to the database: {db_to_connect}")
            return conn, cursor
        except sqlite3.Error as e:
            self.logger.error(f"Database connection error: {e}")
            raise

    def _fetch_image_urls(self, cursor: sqlite3.Cursor) -> List[str]:
        """Fetches non-empty origUrl values from the images table."""
        query = "SELECT origUrl FROM images WHERE origUrl IS NOT NULL AND origUrl != ''"
        try:
            cursor.execute(query)
            rows = cursor.fetchall()
            self.logger.info(f"Fetched {len(rows)} non-empty records from origUrl column.")
            return [row[0] for row in rows]
        except sqlite3.Error as e:
            self.logger.error(f"Query execution error: {e}")
            raise

    def _normalize_url(self, url: str) -> Optional[str]:
        """Normalizes the URL by adding protocol if missing."""
        url = url.strip()
        if not url:
            return None
        if url.startswith("//"):
            return "https:" + url
        if url.startswith(("http://", "https://")):
            return url
        return None  # Invalid URL format

    def _parse_and_classify_urls(self, urls: List[str]) -> Tuple[Counter, Counter]:
        """Parses URLs, extracts domains, and classifies them."""
        main_services: List[str] = []
        other_domains: List[str] = []

        for url in urls:
            normalized_url = self._normalize_url(url)
            if not normalized_url:
                continue

            try:
                parsed = urlparse(normalized_url)
                domain = parsed.netloc.lower()
                if not domain:
                    continue

                service = self.classifier.classify(domain)
                if service == domain:
                    other_domains.append(domain)
                else:
                    main_services.append(service)
            except Exception as e:
                self.logger.warning(f"Failed to parse URL {url}: {e}")
                continue

        return Counter(main_services), Counter(other_domains)

    def analyze(self) -> Tuple[Counter, Counter]:
        """Performs the complete analysis and returns classified counters."""
        conn, cursor = self._connect_to_database()
        try:
            urls = self._fetch_image_urls(cursor)
            main_counter, other_counter = self._parse_and_classify_urls(urls)
            return main_counter, other_counter
        finally:
            conn.close()
            self.logger.info("Database connection closed.")

    def print_results(self, main_counter: Counter, other_counter: Counter, top_other_limit: int = 100) -> None:
        """Prints the analysis results in a formatted manner."""
        print("\nüèÜ –¢–æ–ø-—Å–µ—Ä–≤–∏—Å–æ–≤ (–ø–æ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–º CDN):")
        print("-" * 50)
        for service, count in main_counter.most_common():
            print(f"{service:<30} : {count:>6}")

        print(f"\nüåê –í—Å–µ–≥–æ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤: {len(main_counter)}")

        if other_counter:
            print(f"\nüîç –ü—Ä–æ—á–∏–µ –¥–æ–º–µ–Ω—ã (–Ω–µ –≤–æ—à–µ–¥—à–∏–µ –≤ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏) ‚Äî —Ç–æ–ø-{top_other_limit}:")
            print("-" * 60)
            for domain, count in other_counter.most_common(top_other_limit):
                print(f"{domain:<40} : {count:>4}")
            if len(other_counter) > top_other_limit:
                print(f"... –∏ –µ—â—ë {len(other_counter) - top_other_limit} –¥—Ä—É–≥–∏—Ö –¥–æ–º–µ–Ω–æ–≤.")
            print(f"\nüì¶ –í—Å–µ–≥–æ '–ø—Ä–æ—á–∏—Ö' —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤: {len(other_counter)}")

    def _chunks(self, lst, n):
        """Yield successive n-sized chunks from lst."""
        for i in range(0, len(lst), n):
            yield lst[i:i + n]

    def extract_vk_data_to_new_db(self, new_db_path: str = "vk_images.db", chunk_size: int = 500) -> None:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ –∑–∞–ø–∏—Å–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –í–ö, –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö –≤ –Ω–æ–≤—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç chunking –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –æ—à–∏–±–∫–∏ 'too many SQL variables'.
        """
        conn_old, cursor_old = self._connect_to_database()

        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –ë–î
        conn_new, cursor_new = self._connect_to_database(path=new_db_path)

        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã –≤ –Ω–æ–≤–æ–π –ë–î (—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–∞–∫ –≤ —Å—Ç–∞—Ä–æ–π)
        # images (22 —Å—Ç–æ–ª–±—Ü–∞)
        cursor_new.execute("""
            CREATE TABLE IF NOT EXISTS images (
                id TEXT PRIMARY KEY,
                docid TEXT,
                documentid TEXT,
                reqid TEXT,
                rimId TEXT,
                pos INTEGER,
                url TEXT,
                origUrl TEXT,
                image_url TEXT,
                alt TEXT,
                width INTEGER,
                height INTEGER,
                origWidth INTEGER,
                origHeight INTEGER,
                title TEXT,
                domain TEXT,
                snippet_url TEXT,
                freshness_counter INTEGER,
                is_gif BOOLEAN,
                ecom_shield BOOLEAN,
                censored BOOLEAN,
                loading_state TEXT
            )
        """)
        # image_variants (11 —Å—Ç–æ–ª–±—Ü–æ–≤ + 1 AUTOINCREMENT)
        cursor_new.execute("""
            CREATE TABLE IF NOT EXISTS image_variants (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                image_id TEXT NOT NULL,
                variant_type TEXT NOT NULL,
                url TEXT,
                width INTEGER,
                height INTEGER,
                file_size_bytes INTEGER,
                is_mixed_image BOOLEAN,
                origin_url TEXT,
                origin_width INTEGER,
                origin_height INTEGER,
                FOREIGN KEY(image_id) REFERENCES images(id)
            )
        """)

        # –°–Ω–∞—á–∞–ª–∞ –Ω–∞—Ö–æ–¥–∏–º –≤—Å–µ id –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ —Ç–∞–±–ª–∏—Ü—ã images, –∫–æ—Ç–æ—Ä—ã–µ —Å–≤—è–∑–∞–Ω—ã —Å –í–ö
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º classify –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ origUrl –∏–ª–∏ image_url
        cursor_old.execute("SELECT id, origUrl, image_url FROM images")
        image_rows = cursor_old.fetchall()

        vk_image_ids = set()
        for row in image_rows:
            img_id, orig_url, img_url = row
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º origUrl
            if orig_url:
                norm_orig_url = self._normalize_url(orig_url)
                if norm_orig_url:
                    try:
                        parsed = urlparse(norm_orig_url)
                        domain = parsed.netloc.lower()
                        service = self.classifier.classify(domain)
                        if 'VK' in service:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è 'VK'
                            vk_image_ids.add(img_id)
                            continue  # –ù–∞—à–ª–∏ –ø–æ origUrl, –º–æ–∂–Ω–æ –∏–¥—Ç–∏ –∫ —Å–ª–µ–¥—É—é—â–µ–π –∑–∞–ø–∏—Å–∏
                    except Exception:
                        pass
            # –ï—Å–ª–∏ origUrl –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –ø—Ä–æ–≤–µ—Ä—è–µ–º image_url
            if img_url:
                norm_img_url = self._normalize_url(img_url)
                if norm_img_url:
                    try:
                        parsed = urlparse(norm_img_url)
                        domain = parsed.netloc.lower()
                        service = self.classifier.classify(domain)
                        if 'VK' in service:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è 'VK'
                            vk_image_ids.add(img_id)
                    except Exception:
                        pass

        self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(vk_image_ids)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –í–ö.")

        # –ó–∞—Ç–µ–º –∫–æ–ø–∏—Ä—É–µ–º —ç—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –Ω–æ–≤—É—é –ë–î
        if vk_image_ids:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º set –≤ list –¥–ª—è chunking
            vk_image_ids_list = list(vk_image_ids)
            total_copied_images = 0
            total_copied_variants = 0

            # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ —á–∞–Ω–∫–∞–º
            for chunk_ids in self._chunks(vk_image_ids_list, chunk_size):
                placeholders = ','.join('?' for _ in chunk_ids)
                # –ö–æ–ø–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —á–∞–Ω–∫–∞
                cursor_old.execute(f"SELECT * FROM images WHERE id IN ({placeholders})", chunk_ids)
                chunk_image_data = cursor_old.fetchall()

                if chunk_image_data:
                    insert_images_query = """
                        INSERT INTO images VALUES (
                            ?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?
                        )
                    """
                    cursor_new.executemany(insert_images_query, chunk_image_data)
                    total_copied_images += len(chunk_image_data)
                    self.logger.debug(f"–°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ {len(chunk_image_data)} –∑–∞–ø–∏—Å–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —á–∞–Ω–∫–∞.")

            # –ó–∞—Ñ–∏–∫—Å–∏—Ä—É–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥–ª—è images
            conn_new.commit()
            self.logger.info(f"–í—Å–µ–≥–æ —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ {total_copied_images} –∑–∞–ø–∏—Å–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –Ω–æ–≤—É—é –ë–î.")

            # –¢–µ–ø–µ—Ä—å –∫–æ–ø–∏—Ä—É–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏ –∏–∑ image_variants –ø–æ —á–∞–Ω–∫–∞–º
            for chunk_ids in self._chunks(vk_image_ids_list, chunk_size):
                placeholders = ','.join('?' for _ in chunk_ids)
                cursor_old.execute(f"SELECT * FROM image_variants WHERE image_id IN ({placeholders})", chunk_ids)
                chunk_variant_data = cursor_old.fetchall()

                if chunk_variant_data:
                    insert_variants_query = """
                        INSERT INTO image_variants VALUES (
                            ?,?,?,?,?,?,?,?,?,?,?
                        )
                    """
                    cursor_new.executemany(insert_variants_query, chunk_variant_data)
                    total_copied_variants += len(chunk_variant_data)
                    self.logger.debug(f"–°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ {len(chunk_variant_data)} –∑–∞–ø–∏—Å–µ–π –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —á–∞–Ω–∫–∞.")

            # –ó–∞—Ñ–∏–∫—Å–∏—Ä—É–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥–ª—è image_variants
            conn_new.commit()
            self.logger.info(f"–í—Å–µ–≥–æ —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ {total_copied_variants} –∑–∞–ø–∏—Å–µ–π –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –Ω–æ–≤—É—é –ë–î.")

        else:
            self.logger.info("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –í–ö.")

        conn_old.close()
        conn_new.close()
        self.logger.info(f"–≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –í–ö –∑–∞–≤–µ—Ä—à–µ–Ω. –ù–æ–≤–∞—è –ë–î: {new_db_path}")


def configure_logging() -> None:
    """Configures logging for the application."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )


def main() -> None:
    """Main function to run the image URL analysis."""
    configure_logging()
    logger = logging.getLogger(__name__)

    db_path = 'yandex_images.db'

    try:
        analyzer = ImageUrlAnalyzer(db_path)

        # --- –ù–æ–≤—ã–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª ---
        logger.info("–ù–∞—á–∏–Ω–∞–µ–º —ç–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –í–ö –≤ –Ω–æ–≤—É—é –ë–î...")
        analyzer.extract_vk_data_to_new_db("vk_images.db")
        logger.info("–≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –í–ö –∑–∞–≤–µ—Ä—à–µ–Ω.")

        # --- –°—Ç–∞—Ä—ã–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª ---
        main_counter, other_counter = analyzer.analyze()
        analyzer.print_results(main_counter, other_counter)
    except Exception as e:
        logger.error(f"An error occurred during analysis: {e}")
        return 1

    logger.info("Analysis completed successfully.")
    return 0


if __name__ == "__main__":
    exit(main())
