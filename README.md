# Yandex Reverse Image Search Crawler

Полноценный Python-пайплайн для массового reverse image search в Яндекс.Картинках:

- асинхронный сбор XHR-ответов из браузера через Playwright;
- нормализация результатов в SQLite;
- анализ доменов (в том числе выделение VK-доменов);
- асинхронная загрузка оригиналов;
- очистка базы от битых ссылок;
- дедупликация по точным/приближённым хешам и по имени;
- сортировка изображений по ориентации, мегапикселям и визуальному качеству (MUSIQ/CLIPIQA).

> Проект ориентирован на пакетную офлайн-обработку больших наборов изображений и запуск скриптов по шагам.

---

## Оглавление

1. [Архитектура пайплайна](#архитектура-пайплайна)
2. [Структура репозитория](#структура-репозитория)
3. [Требования](#требования)
4. [Быстрый старт](#быстрый-старт)
5. [Подробный workflow (1 → 7)](#подробный-workflow-1--7)
6. [Схема БД SQLite](#схема-бд-sqlite)
7. [CI/CD и релизный пайплайн](#cicd-и-релизный-пайплайн)
8. [Troubleshooting](#troubleshooting)
9. [Рекомендации по production-использованию](#рекомендации-по-production-использованию)

---

## Архитектура пайплайна

```text
Локальные JPG/JPEG
      │
      ▼
(1) 1_Parse_XHR_Async.py
      │  -> XHR/*.json
      ▼
(2) 2_XHRtoDB.py
      │  -> yandex_images.db (images, image_variants)
      ├─────────────► (3) 3_VK_Domain_Analyzer.py (аналитика доменов)
      ├─────────────► (4) 4_DB_cleanup.py (чистка БД)
      ▼
(5) 5_Download_async.py
      │  -> папка с загруженными изображениями
      ▼
(6) 6_duplicate_finder.py
      │  -> удаление/предпросмотр дубликатов
      ▼
(7.1) ориентация / (7.2) мегапиксели / (7.3) качество
      │
      ▼
Готовый датасет под модерацию, ML или публикацию
```

---

## Структура репозитория

- `1_Parse_XHR_Async.py` — браузерный асинхронный сбор XHR reverse search.
- `2_XHRtoDB.py` — парсинг JSON и импорт в SQLite.
- `3_VK_Domain_Analyzer.py` — анализ распределения доменов, отчёт по VK.
- `4_DB_cleanup.py` — удаление/архивирование невалидных записей в БД.
- `5_Download_async.py` — массовая асинхронная загрузка изображений по URL из БД.
- `6_duplicate_finder.py` — поиск и удаление дубликатов (exact/perceptual/name).
- `7_1_separate_images_by_orientation.py` — сортировка по portrait/landscape/square.
- `7_2_separate_images_by_MegaPixels.py` — сортировка по мегапикселям и стратегиям группировки.
- `7_3_separate_images_by_quality.py` — оценка качества (MUSIQ/CLIPIQA), отчёт и сортировка.
- `requirements.txt` — runtime зависимости проекта.
- `requirements-dev.txt` — runtime + инструменты проверки кода.
- `.github/workflows/ci.yml` — CI (валидация, lint, smoke).
- `.github/workflows/release.yml` — release automation (создание архива исходников).

---

## Требования

### Минимальные

- Python **3.10+** (рекомендовано 3.11);
- Linux/macOS/Windows;
- доступ в интернет для обращения к Яндексу и источникам изображений.

### Python-зависимости

Установка runtime:

```bash
pip install -r requirements.txt
```

Установка dev-инструментов:

```bash
pip install -r requirements-dev.txt
```

### Playwright браузер

После установки зависимостей обязательно установить Chromium для Playwright:

```bash
python -m playwright install chromium
```

### Опционально (ускорение quality-сортировки)

Для `7_3_separate_images_by_quality.py` полезна GPU-версия PyTorch (CUDA).
Если GPU недоступен — скрипт может работать в CPU-режиме, но заметно медленнее.

---

## Быстрый старт

### 1) Подготовьте входные изображения

Скрипт `1_Parse_XHR_Async.py` ищет `.jpg/.jpeg` в текущей директории (`IMAGES_FOLDER = Path('.')`).
Поместите изображения в рабочую папку запуска.

### 2) Сбор XHR

```bash
python 1_Parse_XHR_Async.py
```

Результат: директория `XHR/` с JSON-ответами и `errors.log` при ошибках.

### 3) Импорт в SQLite

```bash
python 2_XHRtoDB.py
```

Результат: `yandex_images.db`.

### 4) (Опционально) Аналитика доменов

```bash
python 3_VK_Domain_Analyzer.py
```

### 5) (Опционально) Чистка БД

```bash
python 4_DB_cleanup.py
```

### 6) Загрузка изображений

```bash
python 5_Download_async.py
```

Скрипт интерактивный: попросит выбрать режим.

### 7) Дедупликация

```bash
python 6_duplicate_finder.py /path/to/images --mode exact --dry-run
python 6_duplicate_finder.py /path/to/images --mode similar --threshold 10
python 6_duplicate_finder.py /path/to/images --mode filename
```

### 8) Постобработка датасета

```bash
python 7_1_separate_images_by_orientation.py
python 7_2_separate_images_by_MegaPixels.py
python 7_3_separate_images_by_quality.py
```

---

## Подробный workflow (1 → 7)

## 1. `1_Parse_XHR_Async.py`

Назначение: загрузить изображения в Yandex Images UI и сохранить релевантные JSON XHR-ответы.

Ключевые параметры в коде:

- `IMAGES_FOLDER` — откуда брать `.jpg/.jpeg`;
- `XHR_FOLDER` — куда писать json;
- `MAX_TABS` — ограничение параллелизма вкладок;
- `HEADLESS` — режим браузера.

Что делает:

1. Ищет все JPG/JPEG в папке.
2. Открывает Yandex Images.
3. Загружает файл в "Поиск по картинке".
4. Переходит на вкладку похожих изображений.
5. Дожимает кнопку "Показать ещё" до конца выдачи.
6. Сохраняет нужные XHR ответы (`serpList/fetch`, `format=json`).

## 2. `2_XHRtoDB.py`

Назначение: превратить пачку XHR JSON в нормализованную SQLite-базу.

Создаёт таблицы:

- `images` — карточка изображения из выдачи;
- `image_variants` — превью/дубли/миниатюры из `viewerData`.

Поведение:

- читает все `XHR/*.json`;
- пропускает битые/невалидные JSON;
- вставляет записи `INSERT OR IGNORE` для основной таблицы.

## 3. `3_VK_Domain_Analyzer.py`

Назначение: аналитика доменной структуры результатов с фокусом на экосистему VK.

Типичные задачи:

- разбор домена из URL;
- подсчёт частот;
- выделение VK-доменов и сравнение с общей выборкой;
- формирование понятного отчёта в консоль/лог.

## 4. `4_DB_cleanup.py`

Назначение: обслуживающая чистка БД.

Сценарии:

- удаление неконсистентных/пустых ссылок;
- подготовка БД перед массовой загрузкой;
- резервное копирование (в зависимости от выбранного сценария скрипта).

## 5. `5_Download_async.py`

Назначение: асинхронно скачать изображения из БД с отслеживанием статусов.

Особенности:

- `aiohttp` + `aiofiles`;
- MIME-aware расширения;
- hash/служебный трекер для уже обработанных файлов;
- устойчивость к таймаутам и сетевым ошибкам.

Практика запуска:

- сначала небольшой прогон на подмножестве;
- затем основной batch;
- затем повторный прогон для добора failed URL.

## 6. `6_duplicate_finder.py`

Поддерживаемые режимы:

- `exact` — точные дубликаты (размер/контент/sha и т.д.);
- `similar` — perceptual hash (`phash/dhash/ahash/whash`) + threshold;
- `filename` — группировка по имени файла.

Полезные флаги:

- `--dry-run` — безопасный предпросмотр без удаления;
- `--interactive` — подтверждение каждого удаления;
- `--max-workers` — ускорение обработки.

## 7. Сортировка и quality-control

### 7.1 `7_1_separate_images_by_orientation.py`

Сортирует по:

- `landscape`
- `portrait`
- `square` (политика квадратных задаётся интерактивно)

### 7.2 `7_2_separate_images_by_MegaPixels.py`

Сортировка по мегапикселям с несколькими стратегиями разбиения,
включая фиксированные диапазоны и более адаптивные подходы.

### 7.3 `7_3_separate_images_by_quality.py`

Оценка качества через `pyiqa` модели:

- MUSIQ;
- CLIPIQA;
- комбинированный режим.

Выход:

- SQLite отчёт по качеству;
- JSON отчёт;
- разложенные папки (`high`, `medium`, `low`).

---

## Схема БД SQLite

Файл: `yandex_images.db`.

### Таблица `images`

Хранит основную сущность результата reverse search:
идентификаторы, URL, размеры, домен, title/snippet, технические флаги.

### Таблица `image_variants`

Хранит варианты отображения для `images.id`:

- `preview`
- `dups`
- `thumb`

Поля включают URL, размеры, file size и origin-метаданные.

---

## CI/CD и релизный пайплайн

В репозитории добавлены workflow:

### 1) `.github/workflows/ci.yml`

Запускается на `push` и `pull_request`:

- проверка синтаксиса всех `*.py` (`compileall`);
- lint (`ruff check .`);
- smoke-проверка, что ключевые скрипты видны в корне.

Это быстрый baseline-контроль без запуска тяжёлых сетевых/GPU-шагов.

### 2) `.github/workflows/release.yml`

Запускается при публикации GitHub Release:

- делает checkout;
- собирает архив исходников проекта;
- прикладывает архив как release-asset.

Таким образом у проекта есть минимально необходимый release-процесс:
верификация в CI + упаковка артефакта при релизе.

---

## Troubleshooting

### Playwright не запускается

1. Убедитесь, что установлен Chromium:

```bash
python -m playwright install chromium
```

2. Для Linux иногда нужны системные пакеты:

```bash
python -m playwright install-deps chromium
```

### Скрипт качества падает по памяти

- Уменьшите размер входных изображений;
- проверьте доступный VRAM/RAM;
- используйте только одну модель (MUSIQ или CLIPIQA);
- при необходимости перейдите на CPU.

### Низкая скорость загрузки

- уменьшите параллелизм/таймауты при нестабильной сети;
- запускайте в несколько проходов;
- предварительно почистите БД от мусорных ссылок.

### Много ложных срабатываний в similar-dedup

- увеличьте `--threshold` для более мягкого сравнения;
- подберите другой hash-метод (`phash` обычно стабильнее);
- используйте `--dry-run` перед удалением.

---

## Рекомендации по production-использованию

- Фиксируйте версию Python в CI и prod-окружении;
- храните промежуточные артефакты (`XHR`, `db backup`, `quality_report.json`);
- регулярно делайте бэкап `yandex_images.db`;
- весь destructive-функционал гоняйте сначала с preview/dry-run;
- отделяйте этап сбора, этап загрузки и этап quality-control по разным job/машинам.

---

## Юридические и этические замечания

- Учитывайте ToS и robots-политику сайтов-источников;
- соблюдайте авторские права и лицензионные ограничения на изображения;
- если обрабатываются персональные данные — обеспечьте соответствие требованиям законодательства.

---

## Лицензия

Если вы планируете публичное использование проекта, добавьте файл `LICENSE`
(например, MIT/Apache-2.0) и зафиксируйте условия использования.
